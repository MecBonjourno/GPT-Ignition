{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guilhermezago/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/whisper/__init__.py:63: UserWarning: /Users/guilhermezago/.cache/whisper/small.en.pt exists, but the SHA256 checksum does not match; re-downloading the file\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████| 461M/461M [01:25<00:00, 5.65MiB/s]\n",
      "/Users/guilhermezago/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model('small.en')\n",
    "result = model.transcribe('ignition.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guilhermezago/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "portuguese_model = whisper.load_model('small')\n",
    "result2 = portuguese_model.transcribe('ignition.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Find general in my contacts list.',\n",
       " 'segments': [{'id': 0,\n",
       "   'seek': 0,\n",
       "   'start': 0.0,\n",
       "   'end': 4.0,\n",
       "   'text': ' Find general in my contacts list.',\n",
       "   'tokens': [50363, 9938, 2276, 287, 616, 13961, 1351, 13, 50563],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.4225467205047607,\n",
       "   'compression_ratio': 0.8048780487804879,\n",
       "   'no_speech_prob': 0.03295033425092697}],\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Fique em general na lista de contatos.',\n",
       " 'segments': [{'id': 0,\n",
       "   'seek': 0,\n",
       "   'start': 0.0,\n",
       "   'end': 4.0,\n",
       "   'text': ' Fique em general na lista de contatos.',\n",
       "   'tokens': [50364,\n",
       "    479,\n",
       "    1925,\n",
       "    846,\n",
       "    2674,\n",
       "    1667,\n",
       "    27764,\n",
       "    368,\n",
       "    660,\n",
       "    26818,\n",
       "    13,\n",
       "    50564],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.9630608191856971,\n",
       "   'compression_ratio': 0.8260869565217391,\n",
       "   'no_speech_prob': 0.03958146274089813}],\n",
       " 'language': 'pt'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sounddevice scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import os\n",
    "\n",
    "def record_audio(filename, duration, fs=44100):\n",
    "    \"\"\"\n",
    "    This function records an audio using microphone and saves it as a .wav file.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The name of the .wav file\n",
    "    duration (int): The duration of the recording in seconds\n",
    "    fs (int): The sample rate of the recording. Default is 44100.\n",
    "    \"\"\"\n",
    "    # Check if the filename extension is .wav\n",
    "    if not filename.endswith('.wav'):\n",
    "        filename += '.wav'\n",
    "    \n",
    "    print(\"Recording for \", duration, \" seconds.\")\n",
    "    # Record audio\n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    print(\"Recording complete. Saving to file.\")\n",
    "    # Save as .wav file\n",
    "    write(filename, fs, recording)  # Save as WAV file \n",
    "    print(\"File saved as: \", os.path.join(os.getcwd(), filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording for  5  seconds.\n",
      "Recording complete. Saving to file.\n",
      "File saved as:  /Users/guilhermezago/Documents/GPT-Vectorized-Analysis/Voice/ignition.wav\n"
     ]
    }
   ],
   "source": [
    "# This will record 5 seconds of audio and save it as 'test.wav'\n",
    "record_audio('ignition', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guilhermezago/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "question = model.transcribe('ignition.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Fique em general na lista de contatos.',\n",
       " 'segments': [{'id': 0,\n",
       "   'seek': 0,\n",
       "   'start': 0.0,\n",
       "   'end': 4.0,\n",
       "   'text': ' Fique em general na lista de contatos.',\n",
       "   'tokens': [50364,\n",
       "    479,\n",
       "    1925,\n",
       "    846,\n",
       "    2674,\n",
       "    1667,\n",
       "    27764,\n",
       "    368,\n",
       "    660,\n",
       "    26818,\n",
       "    13,\n",
       "    50564],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.9630608191856971,\n",
       "   'compression_ratio': 0.8260869565217391,\n",
       "   'no_speech_prob': 0.03958146274089813}],\n",
       " 'language': 'pt'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt3(question, model=\"text-davinci-003\", max_tokens=100):\n",
    "    \"\"\"\n",
    "    This function takes a question string and sends it to GPT-3, which returns a generated response.\n",
    "\n",
    "    Parameters:\n",
    "    question (str): The question to ask GPT-3\n",
    "    model (str): The model to use. Default is \"text-davinci-003\" (as of Sep 2021).\n",
    "    max_tokens (int): The maximum length of the generated response. Default is 100.\n",
    "    \n",
    "    Returns:\n",
    "    str: The generated response from GPT-3.\n",
    "    \"\"\"\n",
    "    response = openai.Completion.create(\n",
    "      engine=model,\n",
    "      prompt=question,\n",
    "      max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_audio('ignition', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = model.transcribe('ignition.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=ask_gpt3(question['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install elevenlabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elevenlabs import generate, play\n",
    "\n",
    "from elevenlabs import set_api_key\n",
    "set_api_key('459d05ac3deb06b480705df76f99ac7a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gennaro.rodrigues@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents.agent_toolkits import ZapierToolkit\n",
    "from langchain.utilities.zapier import ZapierNLAWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guilhermezago/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/langchain/llms/openai.py:170: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/guilhermezago/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/langchain/llms/openai.py:624: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "zapier = ZapierNLAWrapper()\n",
    "toolkit = ZapierToolkit.from_zapier_nla_wrapper(zapier)\n",
    "agent = initialize_agent(toolkit.get_tools(), llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automated_mail_test():\n",
    "    record_audio('ignition', 20)\n",
    "    give_command = model.transcribe('ignition.wav')\n",
    "    print(give_command['text'])\n",
    "    # audio = generate(\n",
    "    #     text=\"I will send an email to Gennaro about it\",\n",
    "    #     voice=\"Bella\",\n",
    "    #     model=\"eleven_monolingual_v1\"   \n",
    "    # )\n",
    "\n",
    "    # play(audio)\n",
    "    answer = agent.run(give_command['text']) \n",
    "    audio2 = generate(\n",
    "        text= answer,\n",
    "        voice=\"Bella\",\n",
    "        model=\"eleven_monolingual_v1\"   \n",
    "    )\n",
    "\n",
    "    play(audio2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_task():\n",
    "    record_audio('ignition', 10)\n",
    "    give_command = model.transcribe('ignition.wav')\n",
    "    print(give_command['text'])\n",
    "    answer = agent.run(give_command['text']) \n",
    "    print(answer)\n",
    "    audio = generate(\n",
    "        text=answer,\n",
    "        voice=\"Bella\",\n",
    "        model=\"eleven_monolingual_v1\"   \n",
    "    )\n",
    "\n",
    "    play(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording for  10  seconds.\n",
      "Recording complete. Saving to file.\n",
      "File saved as:  /Users/guilhermezago/Documents/GPT-Vectorized-Analysis/Voice/ignition.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guilhermezago/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Create an event for May 24th at 12 o'clock and name it Lunch Meeting.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to create an event in Google Calendar.\n",
      "Action: Google Calendar: Quick Add Event\n",
      "Action Input: ['Calendar', 'Describe_Event']\u001b[0m"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request for url: https://nla.zapier.com/api/v1/exposed/01H1274JV4WM6348J0FKRM9C8K/execute/?api_key=sk-ak-GtKYvrsgpMMhVOB1dfIa8TJMUe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m perform_task()\n",
      "Cell \u001b[0;32mIn[77], line 5\u001b[0m, in \u001b[0;36mperform_task\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m give_command \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtranscribe(\u001b[39m'\u001b[39m\u001b[39mignition.wav\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(give_command[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m answer \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mrun(give_command[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]) \n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(answer)\n\u001b[1;32m      7\u001b[0m audio \u001b[39m=\u001b[39m generate(\n\u001b[1;32m      8\u001b[0m     text\u001b[39m=\u001b[39manswer,\n\u001b[1;32m      9\u001b[0m     voice\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBella\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meleven_monolingual_v1\u001b[39m\u001b[39m\"\u001b[39m   \n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/langchain/chains/base.py:213\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    212\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m])[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/langchain/chains/base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/langchain/chains/base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    108\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    109\u001b[0m     inputs,\n\u001b[1;32m    110\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/langchain/agents/agent.py:812\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m--> 812\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m    813\u001b[0m         name_to_tool_map, color_mapping, inputs, intermediate_steps\n\u001b[1;32m    814\u001b[0m     )\n\u001b[1;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m    816\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(next_step_output, intermediate_steps)\n",
      "File \u001b[0;32m~/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/langchain/agents/agent.py:715\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps)\u001b[0m\n\u001b[1;32m    713\u001b[0m         tool_run_kwargs[\u001b[39m\"\u001b[39m\u001b[39mllm_prefix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    714\u001b[0m     \u001b[39m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m     observation \u001b[39m=\u001b[39m tool\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    716\u001b[0m         agent_action\u001b[39m.\u001b[39;49mtool_input,\n\u001b[1;32m    717\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    718\u001b[0m         color\u001b[39m=\u001b[39;49mcolor,\n\u001b[1;32m    719\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_run_kwargs,\n\u001b[1;32m    720\u001b[0m     )\n\u001b[1;32m    721\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     tool_run_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[0;32m~/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/langchain/tools/base.py:73\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_tool_error(e, verbose\u001b[39m=\u001b[39mverbose_)\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_tool_end(\n\u001b[1;32m     75\u001b[0m     observation, verbose\u001b[39m=\u001b[39mverbose_, color\u001b[39m=\u001b[39mcolor, name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m     76\u001b[0m )\n\u001b[1;32m     77\u001b[0m \u001b[39mreturn\u001b[39;00m observation\n",
      "File \u001b[0;32m~/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/langchain/tools/base.py:70\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_tool_start(\n\u001b[1;32m     63\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39m\"\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdescription},\n\u001b[1;32m     64\u001b[0m     tool_input,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(tool_input)\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_tool_error(e, verbose\u001b[39m=\u001b[39mverbose_)\n",
      "File \u001b[0;32m~/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/langchain/tools/zapier/tool.py:121\u001b[0m, in \u001b[0;36mZapierNLARunAction._run\u001b[0;34m(self, instructions)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run\u001b[39m(\u001b[39mself\u001b[39m, instructions: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    120\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Use the Zapier NLA tool to return a list of all exposed user actions.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_wrapper\u001b[39m.\u001b[39;49mrun_as_str(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_id, instructions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n",
      "File \u001b[0;32m~/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/langchain/utilities/zapier.py:141\u001b[0m, in \u001b[0;36mZapierNLAWrapper.run_as_str\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_as_str\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:  \u001b[39m# type: ignore[no-untyped-def]\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Same as run, but returns a stringified version of the JSON for\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39m    insertting back into an LLM.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39mdumps(data)\n",
      "File \u001b[0;32m~/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/langchain/utilities/zapier.py:121\u001b[0m, in \u001b[0;36mZapierNLAWrapper.run\u001b[0;34m(self, action_id, instructions, params)\u001b[0m\n\u001b[1;32m    119\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_action_request(action_id, instructions, params)\n\u001b[1;32m    120\u001b[0m response \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39msend(session\u001b[39m.\u001b[39mprepare_request(request))\n\u001b[0;32m--> 121\u001b[0m response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    122\u001b[0m \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mjson()[\u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Server Error: \u001b[39m\u001b[39m{\u001b[39;00mreason\u001b[39m}\u001b[39;00m\u001b[39m for url: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://nla.zapier.com/api/v1/exposed/01H1274JV4WM6348J0FKRM9C8K/execute/?api_key=sk-ak-GtKYvrsgpMMhVOB1dfIa8TJMUe"
     ]
    }
   ],
   "source": [
    "perform_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording for  20  seconds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m automated_mail_test()\n",
      "Cell \u001b[0;32mIn[63], line 2\u001b[0m, in \u001b[0;36mautomated_mail_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mautomated_mail_test\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     record_audio(\u001b[39m'\u001b[39;49m\u001b[39mignition\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m20\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m     give_command \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtranscribe(\u001b[39m'\u001b[39m\u001b[39mignition.wav\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(give_command[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m, in \u001b[0;36mrecord_audio\u001b[0;34m(filename, duration, fs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m# Record audio\u001b[39;00m\n\u001b[1;32m     20\u001b[0m recording \u001b[39m=\u001b[39m sd\u001b[39m.\u001b[39mrec(\u001b[39mint\u001b[39m(duration \u001b[39m*\u001b[39m fs), samplerate\u001b[39m=\u001b[39mfs, channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m sd\u001b[39m.\u001b[39;49mwait()  \u001b[39m# Wait until recording is finished\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRecording complete. Saving to file.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[39m# Save as .wav file\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/sounddevice.py:395\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ignore_errors)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wait for `play()`/`rec()`/`playrec()` to be finished.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \n\u001b[1;32m    381\u001b[0m \u001b[39mPlayback/recording can be stopped with a `KeyboardInterrupt`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m \n\u001b[1;32m    393\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39mif\u001b[39;00m _last_callback:\n\u001b[0;32m--> 395\u001b[0m     \u001b[39mreturn\u001b[39;00m _last_callback\u001b[39m.\u001b[39;49mwait(ignore_errors)\n",
      "File \u001b[0;32m~/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/sounddevice.py:2601\u001b[0m, in \u001b[0;36m_CallbackContext.wait\u001b[0;34m(self, ignore_errors)\u001b[0m\n\u001b[1;32m   2595\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wait for finished_callback.\u001b[39;00m\n\u001b[1;32m   2596\u001b[0m \n\u001b[1;32m   2597\u001b[0m \u001b[39mCan be interrupted with a KeyboardInterrupt.\u001b[39;00m\n\u001b[1;32m   2598\u001b[0m \n\u001b[1;32m   2599\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2600\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2601\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevent\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m   2602\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2603\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mclose(ignore_errors)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    620\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    623\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "automated_mail_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automated_mail_test_no_recording():\n",
    "    # record_audio('ignition', 10)\n",
    "    give_command = model.transcribe('ignition.wav')\n",
    "    print(give_command['text'])\n",
    "    # audio = generate(\n",
    "    #     text=\"I will send an email to Gennaro Rodrigues about it\",\n",
    "    #     voice=\"Bella\",\n",
    "    #     model=\"eleven_monolingual_v1\"   \n",
    "    # )\n",
    "\n",
    "    # play(audio)\n",
    "    answer = agent.run(give_command['text']) \n",
    "    audio2 = generate(\n",
    "        text=,\n",
    "        voice=\"Bella\",\n",
    "        model=\"eleven_monolingual_v1\"   \n",
    "    )\n",
    "\n",
    "    play(audio2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guilhermezago/Documents/GPT-Vectorized-Analysis/.venv/lib/python3.11/site-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Find Vico in my contacts and send him an email about grocery shopping online.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find Vico's contact information first.\n",
      "Action: Google Contacts: Find Contact\n",
      "Action Input: Search_By: Vico\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{\"name__first\": \"Vico\", \"photo\": \"https://lh3.googleusercontent.com/cm/AOLgnvvUuFmoJ_w4iia61N4tc8iDiaqjGZP54HmGlqE3O8Bv40dXBzodL61G-XxwjQQe=s100\", \"company\": \"Sooply\", \"emails__undefined\": \"vicogrippa@gmail.com\", \"originalId\": \"people/c1619328672299322477\", \"update_time\": \"2023-05-22T18:32:13.343606Z\"}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI found Vico's contact information, now I need to send him an email about grocery shopping online.\n",
      "Action: Gmail: Send Email\n",
      "Action Input: To: vicogrippa@gmail.com, Body: Hi Vico, I wanted to discuss grocery shopping online. Let me know your thoughts and any recommendations you have. Thanks!, Cc: , Subject: Grocery Shopping Online\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m{\"threadId\": \"18844bdba0a2d930\", \"labelIds\": \"SENT\"}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: I found Vico in your contacts and sent him an email about grocery shopping online.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "automated_mail_test_no_recording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
